{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and soil moisture water balance function\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../../PixSWAB_v1')\n",
    "import calibration_tool.pixswab_v1_for_calibration as pixswab\n",
    "import model.hydroStats as hs\n",
    "import datetime\n",
    "import time\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "## Set global parameter\n",
    "global gen\n",
    "gen = 0\n",
    "WarmupDays = 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixswab Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a cleint for distributed computing for files larger than the memory capacity (RAM)\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# import multiprocessing as mp\n",
    "# from dask.distributed import progress\n",
    "\n",
    "# # multiprocessing client\n",
    "# client = pixswab.start_multiprocessing()\n",
    "# client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the config file and prepare inputs\n",
    "config_path = r'..\\config_files\\pixswab_config.ini'\n",
    "pixswab_inputs, pixswab_parameters, log_file_path, out_dir, encoding_output, bsahpe = pixswab.prepare_inputs(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read observed ourflw (discharge)\n",
    "Qobs_file = r'..\\..\\Upper_Litani\\input\\observed_flow\\discharge_Upper_Litani_JoubJannine_beforeQaraoun.csv'\n",
    "Qobs=pd.read_csv(Qobs_file,sep=',',index_col=0)\n",
    "target_column = 'km3/month'\n",
    "Qobs = Qobs[target_column]\n",
    "Qobs.index = pd.to_datetime(Qobs.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_range\n",
    "param_range_file = r'..\\config_files\\ParamRanges.csv'\n",
    "ParamRanges=pd.read_csv(param_range_file,sep=',',index_col=0)\n",
    "# print(ParamRanges)\n",
    "\n",
    "def RunModel(Individual):\n",
    "    # Convert scaled parameter values ranging from 0 to 1 to usncaled parameter values\n",
    "    Parameters = [None] * len(ParamRanges)\n",
    "    for ii in range(0,len(ParamRanges-1)):\n",
    "        Parameters[ii] = Individual[ii]*(float(ParamRanges.iloc[ii,1])-float(ParamRanges.iloc[ii,0]))+float(ParamRanges.iloc[ii,0])\n",
    "    # print(Parameters)\n",
    "    result = pixswab.run_pixswab(pixswab_inputs, Parameters)\n",
    "    Qsim = pixswab.compute_discharge(result, bsahpe, out_dir, log_file_path)\n",
    "    Qsim_target_column = 'discharge'\n",
    "    Qsim = Qsim[Qsim_target_column]\n",
    "    Qsim = Qsim[Qsim.index.isin(Qobs.index)]\n",
    "    KGE = hs.KGE(s=Qsim, o=Qobs, warmup=WarmupDays)\n",
    "    return KGE,\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#   Perform calibration using the DEAP module\n",
    "########################################################################\n",
    "\n",
    "# firstrun = parser.getboolean('Option', 'firstrun')   # using default run as first run\n",
    "# if firstrun:\n",
    "#     para_first = ast.literal_eval(parser.get(\"Option\", \"para_first\"))\n",
    "# bestrun = parser.getboolean('Option', 'bestrun')\n",
    "import array\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "path_calibration_results = r'..\\..\\Upper_Litani\\calibration_results'\n",
    "\n",
    "maximize = True\n",
    "use_multiprocessing = 0\n",
    "pool_limit = 32\n",
    "ngen = 10\n",
    "mu = 16\n",
    "lambda_ = 8\n",
    "\n",
    "firstrun = True\n",
    "para_first = [0.5, 0.5, 2.5, 1.]\n",
    "# pref. flow, groundwater recession, No of run\n",
    "bestrun = True\n",
    "\n",
    "\n",
    "# first standard parameter set\n",
    "# recalculated to a population setting\n",
    "if firstrun:\n",
    "    #para_first = [0.19, 0.53, 6.]\n",
    "    para_first2 = []\n",
    "    for ii in range(0, len(ParamRanges - 1)):\n",
    "        delta = float(ParamRanges.iloc[ii, 1]) - float(ParamRanges.iloc[ii, 0])\n",
    "        if delta == 0:\n",
    "            para_first2.append(0.)\n",
    "        else:\n",
    "            para_first2.append((para_first[ii] - float(ParamRanges.iloc[ii, 0])) / delta)\n",
    "\n",
    "ii = 1\n",
    "\n",
    "\n",
    "if maximize: maxDeap = 1.0\n",
    "else: maxDeap = -1.0\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(maxDeap,))\n",
    "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register(\"attr_float\", random.uniform, 0, 1)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"Individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, len(ParamRanges))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.Individual)\n",
    "\n",
    "def checkBounds(min, max):\n",
    "    def decorator(func):\n",
    "        def wrappper(*args, **kargs):\n",
    "            offspring = func(*args, **kargs)\n",
    "            for child in offspring:\n",
    "                for i in range(len(child)):\n",
    "                    if child[i] > max:\n",
    "                        child[i] = max\n",
    "                    elif child[i] < min:\n",
    "                        child[i] = min\n",
    "            return offspring\n",
    "        return wrappper\n",
    "    return decorator\n",
    "\n",
    "toolbox.register(\"evaluate\", RunModel)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.15)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.3, indpb=0.3)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "toolbox.decorate(\"mate\", checkBounds(0, 1))\n",
    "toolbox.decorate(\"mutate\", checkBounds(0, 1))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "if use_multiprocessing==True:\n",
    "    pool_size = multiprocessing.cpu_count() * 1\n",
    "    print(pool_size, pool_limit)\n",
    "    if pool_size > pool_limit: pool_size = pool_limit\n",
    "    pool = multiprocessing.Pool(processes=pool_size)\n",
    "    toolbox.register(\"map\", pool.map)\n",
    "    print('pool_size:', pool_size)\n",
    "\n",
    "\n",
    "# For someone reason, if sum of cxpb and mutpb is not one, a lot less Pareto optimal solutions are produced\n",
    "cxpb = 0.7\n",
    "mutpb = 0.3\n",
    "\n",
    "startlater = False\n",
    "checkpoint = os.path.join(path_calibration_results,\"checkpoint.pkl\")\n",
    "# checkpoint = r'..\\..\\Upper_Litani\\calibration_results\\checkpoint.pkl'\n",
    "if os.path.exists(os.path.join(checkpoint)):\n",
    "    with open(checkpoint, \"rb\" ) as cp_file:\n",
    "        cp = pickle.load(cp_file)\n",
    "        population = cp[\"population\"]\n",
    "        start_gen = cp[\"generation\"]\n",
    "        random.setstate(cp[\"rndstate\"])\n",
    "        if start_gen > 0:\n",
    "            offspring = cp[\"offspring\"]\n",
    "            halloffame =  cp[\"halloffame\"]\n",
    "            startlater = True\n",
    "            gen = start_gen\n",
    "\n",
    "else:\n",
    "    population = toolbox.population(n=mu)\n",
    "    # Numbering of runs\n",
    "    for ii in range(mu):\n",
    "        population[ii][-1]= float(gen * 1000 + ii+1)\n",
    "    #first run parameter set:\n",
    "    if firstrun:\n",
    "        for ii in range(len(population[0])):\n",
    "            population[0][ii] = para_first2[ii]\n",
    "        population[0][-1] = 0.\n",
    "\n",
    "effmax = np.zeros(shape=(ngen+1,1))*np.NaN\n",
    "effmin = np.zeros(shape=(ngen+1,1))*np.NaN\n",
    "effavg = np.zeros(shape=(ngen+1,1))*np.NaN\n",
    "effstd = np.zeros(shape=(ngen+1,1))*np.NaN\n",
    "if startlater == False:\n",
    "    halloffame = tools.ParetoFront()\n",
    "\n",
    "    # saving population\n",
    "    cp = dict(population=population, generation=gen, rndstate=random.getstate())\n",
    "    with open(checkpoint, \"wb\") as cp_file:\n",
    "        pickle.dump(cp, cp_file)\n",
    "    cp_file.close()\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    halloffame.update(population)\n",
    "    # print('here 1-3')\n",
    "    # Loop through the different objective functions and calculate some statistics\n",
    "    # from the Pareto optimal population\n",
    "\n",
    "    for ii in range(1):\n",
    "        effmax[0,ii] = np.amax([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effmin[0,ii] = np.amin([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effavg[0,ii] = np.average([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effstd[0,ii] = np.std([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "    gen = 0\n",
    "    print(\">> gen: \"+str(gen)+\", effmax_KGE: \"+\"{0:.3f}\".format(effmax[gen,0]))\n",
    "    gen = 1\n",
    "\n",
    "\n",
    "# Begin the generational process\n",
    "conditions = {\"ngen\" : False, \"StallFit\" : False}\n",
    "while not any(conditions.values()):\n",
    "    if startlater == False:\n",
    "        # Vary the population\n",
    "        offspring = algorithms.varOr(population, toolbox, lambda_, cxpb, mutpb)\n",
    "\n",
    "        # put in the number of run\n",
    "        for ii in range(lambda_):\n",
    "            offspring[ii][-1] = float(gen * 1000 + ii + 1)\n",
    "\n",
    "    # saving population\n",
    "    cp = dict(population=population, generation=gen, rndstate=random.getstate(), offspring=offspring, halloffame=halloffame)\n",
    "    with open(checkpoint, \"wb\") as cp_file:\n",
    "        pickle.dump(cp, cp_file)\n",
    "    cp_file.close()\n",
    "    startlater = False\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Update the hall of fame with the generated individuals\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "    # Select the next generation population\n",
    "    population[:] = toolbox.select(population + offspring, mu)\n",
    "\n",
    "    # put in the number of run\n",
    "    #for ii in xrange(mu):\n",
    "    #    population[ii][-1] = float(gen * 1000 + ii + 1)\n",
    "\n",
    "    # Loop through the different objective functions and calculate some statistics\n",
    "    # from the Pareto optimal population\n",
    "    for ii in range(1):\n",
    "        effmax[gen,ii] = np.amax([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effmin[gen,ii] = np.amin([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effavg[gen,ii] = np.average([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "        effstd[gen,ii] = np.std([halloffame[x].fitness.values[ii] for x in range(len(halloffame))])\n",
    "    print(\">> gen: \"+str(gen)+\", effmax_KGE: \"+\"{0:.3f}\".format(effmax[gen,0]))\n",
    "\n",
    "    # Terminate the optimization after ngen generations\n",
    "    if gen >= ngen:\n",
    "        print(\">> Termination criterion ngen fulfilled.\")\n",
    "        conditions[\"ngen\"] = True\n",
    "\n",
    "    gen += 1\n",
    "\n",
    "# Finito\n",
    "if use_multiprocessing == True:\n",
    "    pool.close()\n",
    "elapsed = time.time() - t\n",
    "print(\">> Time elapsed: \"+\"{0:.2f}\".format(elapsed)+\" s\")\n",
    "\n",
    "########################################################################\n",
    "#   Save calibration results\n",
    "########################################################################\n",
    "\n",
    "# Save history of the change in objective function scores during calibration to csv file\n",
    "print(\">> Saving optimization history (front_history.csv)\")\n",
    "front_history = pd.DataFrame({'gen':list(range(gen)),\n",
    "                                  'effmax_R':effmax[:,0],\n",
    "                                  'effmin_R':effmin[:,0],\n",
    "                                  'effstd_R':effstd[:,0],\n",
    "                                  'effavg_R':effavg[:,0],\n",
    "                                  })\n",
    "front_history.to_csv(os.path.join(path_calibration_results,\"front_history.csv\"),',')\n",
    "# as numpy  numpy.asarray  ; numpy.savetxt(\"foo.csv\", a, delimiter=\",\"); a.tofile('foo.csv',sep=',',format='%10.5f')\n",
    "\n",
    "# Compute overall efficiency scores from the objective function scores for the\n",
    "# solutions in the Pareto optimal front\n",
    "# The overall efficiency reflects the proximity to R = 1, NSlog = 1, and B = 0 %\n",
    "front = np.array([ind.fitness.values for ind in halloffame])\n",
    "effover = 1 - np.sqrt((1-front[:,0]) ** 2)\n",
    "best = np.argmax(effover)\n",
    "\n",
    "# Convert the scaled parameter values of halloffame ranging from 0 to 1 to unscaled parameter values\n",
    "paramvals = np.zeros(shape=(len(halloffame),len(halloffame[0])))\n",
    "paramvals[:] = np.NaN\n",
    "for kk in range(len(halloffame)):\n",
    "    for ii in range(len(ParamRanges)):\n",
    "        paramvals[kk][ii] = halloffame[kk][ii]*(float(ParamRanges.iloc[ii,1])-float(ParamRanges.iloc[ii,0]))+float(ParamRanges.iloc[ii,0])\n",
    "\n",
    "# Save Pareto optimal solutions to csv file\n",
    "# The table is sorted by overall efficiency score\n",
    "print(\">> Saving Pareto optimal solutions (pareto_front.csv)\")\n",
    "ind = np.argsort(effover)[::-1]\n",
    "pareto_front = pd.DataFrame({'effover':effover[ind],'R':front[ind,0]})\n",
    "for ii in range(len(ParamRanges)):\n",
    "    pareto_front[\"param_\"+str(ii).zfill(2)+\"_\"+ParamRanges.index[ii]] = paramvals[ind,ii]\n",
    "pareto_front.to_csv(os.path.join(path_calibration_results,\"pareto_front.csv\"),',')\n",
    "\n",
    "# Select the \"best\" parameter set and run Model for the entire forcing period\n",
    "Parameters = paramvals[best,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8139443541372209"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "28a38c5ebd5800cf63447921f43c164575198c8795e8f0ec168080a3b8efe32d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
